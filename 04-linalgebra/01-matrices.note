!type: Basic Math
!deck: Math::Linear Algebra
!tags: Math Linear-Algebra Matrix

!id: 80ab5bc9-54c5-48ea-8edd-878af86130d5
!front: What is a **diagonal matrix**?
!back:
A **diagonal matrix** is a square matrix in which all entries outside the main diagonal are zero. The main diagonal runs from the top left to the bottom right of the matrix.

For example, the following is a \( 3 \times 3 \) diagonal matrix:

\[
\begin{bmatrix}
    a_{11} & 0 & 0 \\
    0 & a_{22} & 0 \\
    0 & 0 & a_{33}
\end{bmatrix}
\]

where \( a_{11} \), \( a_{22} \), and \( a_{33} \) are the diagonal elements.
~~~

!id: b99abfef-10bf-43ff-9d92-7ba5c88d86d3
!front: What is a **upper-trianguar matrix**?
!back:
An **upper-triangular matrix** is a square matrix in which all entries below the main diagonal are zero. The main diagonal runs from the top left to the bottom right of the matrix.

For example, the following is a \( 3 \times 3 \) upper-triangular matrix:

\[
\begin{bmatrix}
    a_{11} & a_{12} & a_{13} \\
    0 & a_{22} & a_{23} \\
    0 & 0 & a_{33}
\end{bmatrix}
\]
~~~

!id: c6424db0-ebd5-47ef-9d74-d6c0c738f3f0
!front: What is a **lower-trianguar matrix**?
!back:
An **lower-triangular matrix** is a square matrix in which all entries above the main diagonal are zero. The main diagonal runs from the top left to the bottom right of the matrix.

For example, the following is a \( 3 \times 3 \) lower-triangular matrix:

\[
\begin{bmatrix}
    a_{11} & 0 & 0 \\
    a_{21} & a_{22} & 0 \\
    a_{31} & a_{32} & a_{33}
\end{bmatrix}
\]
~~~

!id: 63f55d40-ebfc-46e8-85dc-41e7f46efc18
!front: What is the **identity matrix** \( I \)?
!back:
The **identity matrix** \( I \) is a square matrix in which all the elements of the main diagonal are ones, and all other elements are zeros. The main diagonal runs from the top left to the bottom right of the matrix.

For example, the following is a \( 3 \times 3 \) identity matrix:

\[
\begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
\end{bmatrix}
\]

The identity matrix acts as the multiplicative identity in matrix multiplication, meaning that for any matrix \( A \) of compatible size:

\[ AI = IA = A \]
~~~

!id: bae2f751-fbe7-49d2-bbf8-eb7ea957954f
!front: What is a **symmetric matrix**?
!back:
A **symmetric matrix** is a square matrix that is equal to its transpose. In other words, a matrix \( A \) is symmetric if:

\[ A = A^T \]

This means that the elements across the main diagonal are mirror images of each other, i.e., \( a_{ij} = a_{ji} \) for all \( i \) and \( j \).

For example, the following is a \( 3 \times 3 \) symmetric matrix:

\[
\begin{bmatrix}
    1 & 2 & 3 \\
    2 & 4 & 5 \\
    3 & 5 & 6
\end{bmatrix}
\]
~~~

!id: e21aea94-93e7-43b1-ae28-2c2f3a6596c8
!front: What are the multiplication properties of matrices?
!back:
Matrix multiplication follows the distributive law:

\[
\begin{align*}
    A(B \pm C) &= AB \pm AC \\
    (A \pm B)C &= AC \pm BC
\end{align*}
\]

Matrix multiplication follows the associative law:

\[ A(BC) = (AB)C \]

In general, matrix multiplication is not commutative:

\[ AB \ne BA \]

Except when multiplying by the identity matrix \( I \):

\[ AI = IA \]
~~~

!id: 56d3727e-9972-4a71-a87f-afc8331455d4
!front: Is matrix multiplication distributive?
!back:
Yes, matrix multiplication is distributive. For matrices \( A \), \( B \), and \( C \) (of compatible sizes), the following properties hold:

\[
\begin{align*}
    A(B \pm C) &= AB \pm AC \\
    (A \pm B)C &= AC \pm BC
\end{align*}
\]
~~~

!id: 80ccbd19-c4cc-4915-8d89-d15457eb3723
!front: Is matrix multiplication associative?
!back:
Yes, matrix multiplication is associative. For matrices \( A \), \( B \), and \( C \) (of compatible sizes), the following properties hold:

\[ A(BC) = (AB)C \]
~~~

!id: 46191270-ee52-41f9-bc2c-bb96aff1beef
!front: Is matrix multiplication commutative?
!back:
No, matrix multiplication is *not* commutative. For any matrices \( A \) and \( B \):

\[ AB \ne BA \]

Except when multiplying by the identity matrix \( I \):

\[ AI = IA \]
~~~

!id: 97497647-99db-4018-b980-3654c4bcdadd
!front: What is the inverse of a square matrix?
!back:
The inverse of a square matrix \( A \) is a square matrix \( A^{-1} \) such that:

\[ A \cdot A^{-1} = A^{-1} \cdot A = I \]

where \( I \) is the identity matrix of the same size as \( A \).
~~~

!id: 6be199f5-db3d-4de8-8e93-2b1a5d3ddfb1
!front: What is a singular square matrix?
!back:
A square matrix \( A \) is singular if its determinant is zero:

\[ \det(A) = 0 \]

A singular square matrix \( A \) cannot be inverted, meaning \( A^{-1} \) does not exist.
~~~

!id: 90ad53c2-36b2-49a5-a963-f9455d43a5d5
!front: When a matrix does not have an inverse?
!back:
A matrix \( A \) does not have an inverse if:

- It is not square.
- It is singular, meaning its determinant is zero \( \det(A) = 0 \).
~~~

!id: c5a041d8-7fbe-425d-8e98-8ca49f0bbdec
!front: When the determinant of a matrix is zero?
!back: A square matrix has zero determinant when its rows or columns are linearly dependent - meaning one row (or column) can be written as a linear combination of the others.
~~~

!id: 662a1e6c-360f-4469-af0c-4f849be6b786
!front: What is the geometric interpretation of the absolute determinant value of a matrix?
!back:
For a square matrix \( A \), the absolute value of the determinant \( |\det(A)| \) is the factor by which the matrix scales
\( n \)-dimensional volume:

- \( | \det(A) | = 0 \): the transformation flattens space into a lower dimension (e.g., collapsing area or volume to zero).
- \( | \det(A) | = 1 \): the transformation preserves area/volume (it may rotate, reflect, or shear, but doesn't scale).
- \( | \det(A) | > 1 \): volumes are expanded.
- \( | \det(A) | < 1 \): volumes are contracted.
~~~

!id: 67a590c4-0848-44bb-b2fe-55611a03a4e6
!front: What is the geometric interpretation of the zero determinant of a matrix?
!back: A determinant is zero whenever the transformation collapses space into a lower dimension.
~~~

!id: 09cce54a-d651-4939-83f9-7b48830a42f4
!front: What is the determinant of a triangular matrix?
!back:
The determinant of a triangular matrix is the product of its diagonal entries.

For a diagonal matrix:

\[
\begin{align*}
    \det(A) &=
    \begin{vmatrix}
        a_{11} & 0 & \cdots & 0 \\
        0 & a_{22} & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & a_{nn}
    \end{vmatrix} \\
    &= \prod_{i=1}^n a_{ii} \\
    &= a_{11} \cdot a_{22} \cdot a_{33} \cdots a_{nn}
\end{align*}
\]

For an upper-triangular matrix:

\[
\begin{align*}
    \det(A) &=
    \begin{vmatrix}
        a_{11} & a_{12} & \cdots & a_{nn} \\
        0 & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & a_{nn}
    \end{vmatrix} \\
    &= \prod_{i=1}^n a_{ii} \\
    &= a_{11} \cdot a_{22} \cdot a_{33} \cdots a_{nn}
\end{align*}
\]

For a lower-triangular matrix:

\[
\begin{align*}
    \det(A) &=
    \begin{vmatrix}
        a_{11} & 0 & \cdots & 0 \\
        a_{21} & a_{22} & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{vmatrix} \\
    &= \prod_{i=1}^n a_{ii} \\
    &= a_{11} \cdot a_{22} \cdot a_{33} \cdots a_{nn}
\end{align*}
\]
~~~

!id: a7899052-7d92-4cbb-81f5-61ce0bab17a5
!front: When the determinant of a triangular matrix is zero?
!back: The determinant of a triangular matrix is zero whenever one of its diagonal entries is zero.
~~~

!id: acc466b6-01ef-42f9-bcfb-a4a6283d1163
!front: What is the determinant of a \( 2 \times 2 \) matrix \( A \)?
!back:
Given a \( 2 \times 2 \) matrix \( A = \begin{bmatrix}
   a & b \\
   c & d
\end{bmatrix} \) its determinant is computed as:

\[
\begin{align*}
    \det(A) &=
    \begin{vmatrix}
        a & b \\
        c & d
    \end{vmatrix} \\
    &= ad - bc
\end{align*}
\]
~~~

!id: 8a705ad6-8eba-4085-98f9-56b39aaa4c47
!front: What is the determinant of a \( 3 \times 3 \) matrix \( A \)?
!back:
Given a \( 3 \times 3 \) matrix \( A = \begin{bmatrix}
    a & b & c \\
    d & e & f \\
    g & h & i
\end{bmatrix} \) its determinant is computed as:

\[
\begin{align*}
    \det(A) &=
    \begin{vmatrix}
        a & b & c \\
        d & e & f \\
        g & h & i
    \end{vmatrix} \\
    &= a
    \begin{vmatrix}
        e & f \\
        h & i
    \end{vmatrix} - b
    \begin{vmatrix}
        d & f \\
        g & i
    \end{vmatrix} + c
    \begin{vmatrix}
        d & e \\
        g & h
    \end{vmatrix} \\
    &= a(ei - fh) - b(di - fg) + c(dh - eg)
\end{align*}
\]
~~~

!id: f97aa793-7bc9-43f1-b0c0-f54e5af6332e
!front: What is the minor \( M_{ij} \) of the entry \( a_{ij} \) in a matrix \( A \)?
!back:
The minor \( M_{ij} \) of the entry \( a_{ij} \) in a matrix \( A \) is the determinant of the remaining entries when the row and column containing \( a_{ij} \) are removed.

Example:

For the matrix \( A = \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
\end{bmatrix} \), the minor \( M_{22} \) of the entry \( a_{22} = 5 \) is computed by removing the second row and second column, resulting in the submatrix \( \begin{bmatrix}
    1 & 3 \\
    7 & 9
\end{bmatrix} \):

\[
M_{22} = \begin{vmatrix}
    1 & 3 \\
    7 & 9
\end{vmatrix}
\]
~~~

!id: af5d999c-45f1-45d5-9388-f3a9d9ccfed6
!front: What is the cofactor \( C_{ij} \) of the entry \( a_{ij} \) in a matrix \( A \)?
!back:
The cofactor \( C_{ij} \) of the entry \( a_{ij} \) is in a matrix \( A \) is given by:

\[ C_{ij} = (-1)^{i + j} \cdot M_{ij} \]

where \( M_{ij} \) is the minor of the entry \( a_{ij} \).

Example:

For the matrix \( A = \begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
\end{bmatrix} \), the cofactor \( C_{21} \) of the entry \( a_{21} = 4 \) is

\[
\begin{align*}
C_{ij} &= (-1)^{2+1} \cdot M_{21} \\
&= - \begin{vmatrix}
    2 & 3 \\
    8 & 9
\end{vmatrix}
\end{align*}
\]

Cofactors are a useful tool in helping us to compute determinants of larger matrices.
~~~

!id: b9fbcc42-e985-4c80-9103-b5dcde06a992
!front: How to use the cofactor expansion to compute the determinant of a square matrix \( A \)?
!back:
The determinant of a square matrix \( A \) can be computed using cofactors by expanding along any row or column.

For example, the cofactor expansion across the first row \( i=1 \) is:

\[
\begin{align*}
    \det(A) &= \sum_{j=1}^{n} a_{1j} C_{1j} \\
    &= a_{11} C_{11} + a_{12} C_{12} + \cdots + a_{1n} C_{1n}
\end{align*}
\]

Or, the cofactor expansion across the second column \( j=2 \) is:

\[
\begin{align*}
    \det(A) &= \sum_{i=1}^{m} a_{i2} C_{i2} \\
    &= a_{12} C_{12} + a_{22} C_{22} + \cdots + a_{m2} C_{m2}
\end{align*}
\]
~~~

!id: 3705cf56-6fd7-4178-b000-293e07490b99
!front: What is the Laplace expansion to compute the determinant of a square matrix \( A \)?
!back: The Laplace expansion to compute the determinant is the cofactor expansion along any row or column.
~~~

!id: bf632e4d-d348-4722-a6c9-c9283e3294f8
!front: What is the inverse of a \( 2 \times 2 \) matrix \( A \)?
!back:
Given a \( 2 \times 2 \) matrix \( A = \begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix} \), its inverse \( A^{-1} \) is computed as:

\[
A^{-1} = \frac{1}{\det(A)} \begin{bmatrix}
    d & -b \\
    -c & a
\end{bmatrix} = \frac{1}{ad - bc} \begin{bmatrix}
    d & -b \\
    -c & a
\end{bmatrix}
\]
~~~

!id: b4e1a0cc-7e35-4d1d-b9d6-e5048208934e
!front: What are the properties of symmetric matrices?
!back:
Suppose that is \( A \) symmetric matrix. Then, we have the following properties:

- \( -A \) is symmetric.
- \( cA \) is symmetric for any \( c \ne 0 \).
- \( A^m \) is symmetric for any positive integer \( m \).
- \( A^{-1} \) is symmetric for nonsingular \( A \).

If \( A \) and \( B \) symmetric matrices, we have the following properties:

- The sum \( A + B \) is symmetric.
- The product \( A \cdot B \) is *not* always symmetric.

Finally:

- \( A + A^T \) is symmetric for any *square* matrix \( A \).
- \( A \cdot A^T \) and \( A^T \cdot A \) is symmetric for *any* \( m \times n \) matrix \( A \).

Any diagonal matrix is symmetric.
~~~

!id: 6265ef72-9bad-4c48-b490-24446bcfc10b
!front: If \( A \) is a symmetric matrix, is \( A^{m} \) symmetric?:
!back: Yes, \( A^{m} \) is symmetric for a symmetric matrix \( A \) and a positive integer \( m \).
~~~

!id: d0475616-146a-47a8-bcc1-2fd19442dd19
!front: If \( A \) is a symmetric matrix, is \( A^{-1} \) symmetric?:
!back: Yes, \( A^{-1} \) is symmetric for a nonsingular symmetric matrix \( A \).
~~~

!id: 768793db-4e6a-4695-9954-955a163d3e38
!front: If \( A \) and \( B \) are symmetric matrices, is their sum \( A + B \) symmetric?:
!back: Yes, the sum \( A + B \) is symmetric for two symmetric matrices \( A \) and \( B \).
~~~

!id: dceba5de-f019-4dc7-a946-190b07e8de5a
!front: If \( A \) and \( B \) are symmetric matrices, is their product \( A \cdot B \) symmetric?:
!back: No, the product \( A \cdot B \) is *not* always symmetric.
~~~

!id: 495efd7f-36de-467e-92b2-d50d0d9034f6
!front: For which matrix \( A \) the sum \( A + A^T \) is symmetric?
!back: The sum \( A + A^T \) is symmetric for *any square* \( n \times n \) matrix \( A \).
~~~

!id: 0894c97c-ab4a-4e1e-a499-775d480792bf
!front: For which matrix \( A \) the product \( A \cdot A^T \) is symmetric?
!back: The product \( A \cdot A^T \) is symmetric for *any rectangular* \( m \times n \) matrix \( A \).
~~~

!id: 8e0ae878-8bed-44b5-8b7f-9586cda93890
!front: When a diagonal matrix is symmetric?
!back: *Any* diagonal matrix is symmetric.
~~~

!id: 536075ed-e030-4224-b00c-7ad83cf77987
!front: What is an **eigenvector** \( \mathbf{v} \) of a square matrix \( A \)?
!back:
Given a matrix \( A \) an **eigenvector** of \( A \) is a *non-zero* vector \( \mathbf{v} \) such that

\[ A \mathbf{v} = \lambda \mathbf{v} \]

for some scalar \( \lambda \in \mathbb{R} \), \( \lambda \ne 0 \). The scalar \( \lambda \) is called the **eigenvalue** corresponding to the eigenvector \( \mathbf{v} \).
~~~
